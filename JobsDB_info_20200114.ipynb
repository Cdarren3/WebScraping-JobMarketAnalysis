{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "#data container, storing every rows/wordcounts for each sub pages\n",
    "#to be changed to DataFrame\n",
    "dict_title_additional_info = {'Job Title':[], 'Career Level':[],'Qualification':[],'Years of Experience':[], \"Industry\":[]}\n",
    "dict_wordcount = {}\n",
    "url = 'https://hk.jobsdb.com/hk/en/Search/FindJobs?JSRV=1&JobCat=311%2C134%2C141&page='\n",
    "html = requests.get(url + str(page))\n",
    "soup = bs(html.text, 'html.parser')\n",
    "total_pg = int(re.sub(r'\\D','', soup.find(class_='sQuda_6').get_text())) // 30 + 1 #divided by 30 since one page has 30 sublink\n",
    "\n",
    "\n",
    "# while page < 5: (for code testing)\n",
    "while page < total_pg:\n",
    "    links = []\n",
    "    url1 = url + str(page)\n",
    "    html = requests.get(url1)\n",
    "#     time.sleep(10) (optional)\n",
    "    soup = bs(html.text, 'html.parser')\n",
    "    x = soup.find_all(class_=\"FYwKg _1GAuD C6ZIU_6 _6ufcS_6 _27Shq_6 sQuda_6 _2WTa0_6\")#scrape all subpages' link\n",
    "    for i in x:\n",
    "        links.append(i.a['href'])\n",
    "\n",
    "    for i in links:\n",
    "        #l is list of links in one page\n",
    "        #i is every sub pages. \n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path='C:/Users/USER/OneDrive/Xccelerate/chromedriver.exe')\n",
    "        driver.get(i)\n",
    "        subhtml = driver.page_source\n",
    "        soup_post = bs(subhtml, \"html.parser\")\n",
    "        #post = requests.get(i)\n",
    "        #soup_post = bs(post.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        #Codes of What we scraped start here#\n",
    "        \n",
    "        #Job title\n",
    "        z = soup_post.find_all(class_='FYwKg C6ZIU_6 _3nVJR_6 _642YY_6 _27Shq_6 _2k6I7_6')\n",
    "        dict_title_additional_info['Job Title'].append(z)\n",
    "        \n",
    "        #Additional info\n",
    "        y = soup_post.find_all(class_='FYwKg _32Ekc _2fqoM_6 _1hqiH_6')\n",
    "\n",
    "        for i in y: #data cleaning\n",
    "            dict_title_additional_info['Career Level'].append(y[0].text.replace('Career Level',''))\n",
    "            dict_title_additional_info['Qualification'].append(y[1].text.replace('Qualification',''))\n",
    "            dict_title_additional_info['Years of Experience'].append(y[2].text.replace('Years of Experience',''))\n",
    "            dict_title_additional_info['Industry'].append(y[-1].text.replace('Industry',''))  \n",
    "        \n",
    "        \n",
    "    print(dict_title_additional_info)\n",
    "    page += 1\n",
    "\n",
    "df_title_additional_info = pd.DataFrame.from_dict(dict_title_additional_info)\n",
    "df_title_additional_info.to_csv('df_info_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
